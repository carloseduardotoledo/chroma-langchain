{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Question Answering with local persistence\n",
    "\n",
    "An example of using Chroma DB and LangChain to do question answering over documents, with a locally persisted database. \n",
    "You can store embeddings and documents, then use them again later.\n",
    "\n",
    "__This example was build with the following package versions:__\n",
    "\n",
    "chromadb Version: 0.4.22\n",
    "\n",
    "openai Version: 1.7.2\n",
    "\n",
    "langchain Version: 0.1.0\n",
    "* Since VectorDBQA is also deprecated in the updated packege version this notebook uses RetrievalQA from langchain.chains in its place.\n",
    "\n",
    "langchain-openai Version: 0.0.2.post1\n",
    "* Since langchain_community is depracated and will be removed in next package version this notebook uses langchain-openai in its place.\n",
    "\n",
    "langchain-core Version: 0.1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports from langchain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "#Imports from langchain_openai\n",
    "from langchain_openai import OpenAI \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#Imports from langchain_core\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "#Imports from Chroma\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safely Use the OpenAI API Key Without Exposing it in Your Code\n",
    "https://medium.com/@itsanirudhjoshi/how-to-safely-use-the-openai-api-key-without-exposing-it-in-your-code-%EF%B8%8F-setting-it-as-an-a10cccbb9a7f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Access the API key from the environment variable\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Chroma DB with local persistence\n",
    "\n",
    "Creating a database named TheWhiteHouse_db and a particular collection named POTUS_Speeches_and_Remarks. As an example of having one database of a broader subject and quering only a particular document collection for an answer.  The same database coud have other collections for different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"TheWhiteHouse_db\"\n",
    "collection_name   = \"POTUS_Speeches_and_Remarks\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=database_name) \n",
    "\n",
    "collection = chroma_client.get_or_create_collection(collection_name)\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "#defines \"POTUS_Speeches_and_Remarks\" collection from \"TheWhiteHouse_db\" database as the context source\n",
    "langchain_chroma = Chroma(                                \\\n",
    "    client=chroma_client,                                 \\\n",
    "    collection_name=collection_name, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process documents\n",
    "\n",
    "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace.\n",
    "\n",
    "Next we split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the text\n",
    "loader = TextLoader('state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add documentos to Collection \"POTUS_Speeches_and_Remarks\" into \"TheWhiteHouse_db\" local database\n",
    "# Create embeddings for each chunk and insert into the Chroma vector database\n",
    "# thereis is no need to call persist() as would be necessary in previous versions of ChromaDB\n",
    "langchain_chroma.add_documents(documents=texts)\n",
    "\n",
    "langchain_chroma = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Database from disk, and create the chain\n",
    "Be sure to pass the same `path`, `collection_name` and `embedding_function` as you did when you instantiated the database. Initialize the chain we will use for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "langchain_chroma = Chroma(                                \\\n",
    "    client=chroma_client,                                 \\\n",
    "    collection_name=collection_name, embedding_function=embedding_function)\n",
    "\n",
    "llm=OpenAI()\n",
    "\n",
    "# Using langchain_core.VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=langchain_chroma)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions!\n",
    "\n",
    "Now we can use the chain to ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
       " 'result': ' The President praised Ketanji Brown Jackson as a highly qualified nominee for the United States Supreme Court, who will continue the legacy of excellence of retiring Justice Stephen Breyer. He also mentioned her background as a former litigator, federal public defender, and member of a family of public school educators and police officers. He also mentioned her broad range of support from various groups and individuals.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "# `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "When you're done with the database, you can delete it from disk. You can delete the specific collection you're working with (if you have several), or delete the entire database by nuking the persistence directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cleanup, you can delete the collection\n",
    "chroma_client = chromadb.PersistentClient(path=database_name) \n",
    "chroma_client.delete_collection(collection_name)\n",
    "\n",
    "# Or just nuke the persist directory\n",
    "#!rm -rf TheWhiteHouse_db/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c909e91d0cd7642213937968dfc91c71973575965f56cdcabb1e0b29abe5f7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
